{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. System Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **RAM:** *30GB*\n",
    "- **CPU:** *12 Core - 24 thread Thread Reaper, INTEL Xenon*\n",
    "- **GPU:** *Nvidia 1080 Ti - 11GB vRAM* + *Nvidia Titan XP - 12GB vRAM* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag Of Words.ipynb  data  naive-babes-sklearn.ipynb  Testing.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the datasetÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect('./data/reviewsV1.db') as conn:\n",
    "    data = pd.read_sql_query('SELECT * FROM Review', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('index', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Some Legacy Data Cleaning that causes problem in Word2Vec modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.index != 258456]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Time Based Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by='Time', inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "TRAIN_SIZE = int(data.shape[0] * 0.7)\n",
    "TEST_SIZE = data.shape[0] - TRAIN_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254882"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109236"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data[0: TRAIN_SIZE]\n",
    "data_test = data[TRAIN_SIZE:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Check if the Splitting was performed properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(data_train.shape[0] == TRAIN_SIZE)\n",
    "assert(data_test.shape[0] == TEST_SIZE)\n",
    "assert(data.Time.max() == data_test.Time.reset_index(drop=True)[TEST_SIZE -1])\n",
    "assert(data.Time.min() == data_train.Time.reset_index(drop=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training BagOfWords Model on data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Creating a BoW on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cunt = CountVectorizer(binary=True, max_features=5000)  # For Bernouli's Naive Bayes\n",
    "cunt.fit(data_train.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(254882, 5000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dtrain = cunt.transform(data_train.Text)\n",
    "\n",
    "Dtrain.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dtrain = Dtrain.toarray()  # Almost 10.2 GB of RAM usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Creating a BoW on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109236, 5000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dtest = cunt.transform(data_test.Text)\n",
    "Dtest.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4369440000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "109236 * 5000 * 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dtest = Dtest.toarray()  # Almost 4.3 GB of RAM usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(any(Dtrain[0] == 1) == True)\n",
    "assert(any(Dtest[0] == 1) == True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Scoring parameters*\n",
    "\n",
    "**http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "['accuracy',\n",
    " 'adjusted_mutual_info_score',\n",
    " 'adjusted_rand_score',\n",
    " 'average_precision',\n",
    " 'completeness_score',\n",
    " 'explained_variance',\n",
    " 'f1',\n",
    " 'f1_macro',\n",
    " 'f1_micro',\n",
    " 'f1_samples',\n",
    " 'f1_weighted',\n",
    " 'fowlkes_mallows_score',\n",
    " 'homogeneity_score',\n",
    " 'mutual_info_score',\n",
    " 'neg_log_loss',\n",
    " 'neg_mean_absolute_error',\n",
    " 'neg_mean_squared_error',\n",
    " 'neg_mean_squared_log_error',\n",
    " 'neg_median_absolute_error',\n",
    " 'normalized_mutual_info_score',\n",
    " 'precision',\n",
    " 'precision_macro',\n",
    " 'precision_micro',\n",
    " 'precision_samples',\n",
    " 'precision_weighted',\n",
    " 'r2',\n",
    " 'recall',\n",
    " 'recall_macro',\n",
    " 'recall_micro',\n",
    " 'recall_samples',\n",
    " 'recall_weighted',\n",
    " 'roc_auc',\n",
    " 'v_measure_score']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to optimize the $\\alpha$ parameter in the Laplace Smoothing using F1 score but however we are going the report the following metrics in each iterations.\n",
    "\n",
    "- Accuracy\n",
    "- F1 score\n",
    "- Recall\n",
    "- Precison\n",
    "- Confusion Matrix\n",
    "- Training F1 score\n",
    "- Valiation F1 score\n",
    "- Training Recall\n",
    "- Validation Recall\n",
    "- Training Precision\n",
    "- Validation Precision\n",
    "- Average Fitting Time\n",
    "- Average Prediction time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLDS = 10\n",
    "scorings = ['accuracy', 'f1', 'recall', 'precision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BC_Kfold_cross_validation(\n",
    "    clf,\n",
    "    features,\n",
    "    labels,\n",
    "    scorings,\n",
    "    cv,\n",
    "    test_size,\n",
    "    shuffle,\n",
    "    **kwargs):\n",
    "    \"\"\"\n",
    "    Perform Kfold cross validation for a Binary Classifier.\n",
    "    \n",
    "    :param clf: Classifier to use for fitting the data\n",
    "    :param features: Features for the dataset. Expected np.matrix, np.array\n",
    "    :param labels: Labels for the dataset. Expected np.array, list\n",
    "    :param scorings: Scoring algorithms to evaluate\n",
    "    :param test_size: Splitting ratio while splitting the dataset for train and CV.\n",
    "    :param cv: Number of iterations cross validation is perfomed.\n",
    "    :param shuffle: Shuffle the data each time for cross validations.\n",
    "    \"\"\"\n",
    "    table = kwargs.get('ptable')\n",
    "    field_names=[\n",
    "        \"fit_time\",\n",
    "        \"score_time\",\n",
    "        \"train_accuracy\",\n",
    "        \"cv_accuracy\",\n",
    "        \"train_f1\",\n",
    "        \"cv_f1\",\n",
    "        \"train_recall\",\n",
    "        \"cv_recall\",\n",
    "        \"train_precision\",\n",
    "        \"cv_precision\"\n",
    "    ]\n",
    "    if shuffle:\n",
    "        from sklearn.model_selection import ShuffleSplit\n",
    "        CV = ShuffleSplit(n_splits=cv, test_size=test_size, random_state=42)\n",
    "    else:\n",
    "        CV = cv\n",
    "        \n",
    "    scores = cross_validate(\n",
    "        clf,\n",
    "        features,\n",
    "        labels,\n",
    "        scoring=scorings,\n",
    "        cv=CV,\n",
    "        verbose=2,\n",
    "        return_train_score=True)\n",
    "    \n",
    "    ## Formatted strings would only work in python >= 3.6\n",
    "    table_row = []\n",
    "    # Report the timings\n",
    "    table_row.append(round(scores['fit_time'].mean(), 5))\n",
    "    table_row.append(round(scores['score_time'].mean(), 5))\n",
    "    table_row.append(round(scores['train_accuracy'].mean(), 5))\n",
    "    table_row.append(round(scores['test_accuracy'].mean(), 5))\n",
    "    table_row.append(round(scores['train_f1'].mean(), 5))\n",
    "    table_row.append(round(scores['test_f1'].mean(), 5))\n",
    "    table_row.append(round(scores['train_recall'].mean(), 5))\n",
    "    table_row.append(round(scores['test_recall'].mean(), 5))\n",
    "    table_row.append(round(scores['train_precision'].mean(), 5))\n",
    "    table_row.append(round(scores['test_precision'].mean(), 5))\n",
    "    hparams = kwargs.get(\"hyperparams\", None)\n",
    "    if hparams:\n",
    "        for each in hparams:\n",
    "            field_names.append(each.upper())\n",
    "            table_row.append(clf.get_params()[each])\n",
    "    table.field_names = field_names\n",
    "    table.add_row(table_row)\n",
    "    print(f'Done for {clf}')\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_params_with_ALPHA(ALPHAS, features, labels, scorings, cv, **kwargs):\n",
    "    \"\"\"\n",
    "    Fit KNN with all combination of ALGO and NEIGHBOURS\n",
    "    in each iterrations.\n",
    "    \n",
    "    :param ALGO: List of algorithms to try\n",
    "    :param NEIGHBOURS: List of neighbours to try\n",
    "    \"\"\"\n",
    "    table = PrettyTable()\n",
    "    for ALPHA in ALPHAS:\n",
    "        clf = MultinomialNB(alpha=ALPHA)\n",
    "        test_size = kwargs.get('test_size', 0.3)\n",
    "        shuffle = kwargs.get('shuffle', True)\n",
    "        hparams = kwargs.get('hyperparams')\n",
    "        table = BC_Kfold_cross_validation(clf,\n",
    "                                            features,\n",
    "                                            labels,\n",
    "                                            scorings,\n",
    "                                            cv,\n",
    "                                            test_size,\n",
    "                                            shuffle,\n",
    "                                            hyperparams=hparams,\n",
    "                                            ptable=table)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Dtrain\n",
    "labels = data_train.Polarity.apply(lambda x: 1 if x == 'positive' else 0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(254882, 5000)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(254882,)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  19.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   38.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  19.9s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  20.3s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  20.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  19.9s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  20.2s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  20.2s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  20.3s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  20.1s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  20.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  6.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for MultinomialNB(alpha=0.001, class_prior=None, fit_prior=True)\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  19.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   38.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  20.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  19.9s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  19.8s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  19.9s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  19.5s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  19.3s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  19.9s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  20.0s\n",
      "[CV]  ................................................................\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.001, 0.01, 0.1, 1, 10]\n",
    "table = fit_params_with_ALPHA(alphas, features, labels, \n",
    "                              scorings, NUM_FOLDS, hyperparams=['alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>fit_time</th>\n",
       "        <th>score_time</th>\n",
       "        <th>train_accuracy</th>\n",
       "        <th>cv_accuracy</th>\n",
       "        <th>train_f1</th>\n",
       "        <th>cv_f1</th>\n",
       "        <th>train_recall</th>\n",
       "        <th>cv_recall</th>\n",
       "        <th>train_precision</th>\n",
       "        <th>cv_precision</th>\n",
       "        <th>ALPHA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>12.03081</td>\n",
       "        <td>8.02428</td>\n",
       "        <td>0.90058</td>\n",
       "        <td>0.89638</td>\n",
       "        <td>0.94182</td>\n",
       "        <td>0.93938</td>\n",
       "        <td>0.9456</td>\n",
       "        <td>0.94406</td>\n",
       "        <td>0.93806</td>\n",
       "        <td>0.93474</td>\n",
       "        <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>11.95605</td>\n",
       "        <td>7.86128</td>\n",
       "        <td>0.90057</td>\n",
       "        <td>0.89636</td>\n",
       "        <td>0.94181</td>\n",
       "        <td>0.93937</td>\n",
       "        <td>0.9456</td>\n",
       "        <td>0.94404</td>\n",
       "        <td>0.93806</td>\n",
       "        <td>0.93474</td>\n",
       "        <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>11.88452</td>\n",
       "        <td>7.91433</td>\n",
       "        <td>0.90052</td>\n",
       "        <td>0.89634</td>\n",
       "        <td>0.94179</td>\n",
       "        <td>0.93935</td>\n",
       "        <td>0.94555</td>\n",
       "        <td>0.94401</td>\n",
       "        <td>0.93805</td>\n",
       "        <td>0.93474</td>\n",
       "        <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>11.93363</td>\n",
       "        <td>7.92629</td>\n",
       "        <td>0.90016</td>\n",
       "        <td>0.89605</td>\n",
       "        <td>0.94157</td>\n",
       "        <td>0.93918</td>\n",
       "        <td>0.9453</td>\n",
       "        <td>0.94376</td>\n",
       "        <td>0.93787</td>\n",
       "        <td>0.93464</td>\n",
       "        <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>11.88802</td>\n",
       "        <td>7.95451</td>\n",
       "        <td>0.89793</td>\n",
       "        <td>0.89444</td>\n",
       "        <td>0.94045</td>\n",
       "        <td>0.93842</td>\n",
       "        <td>0.94704</td>\n",
       "        <td>0.94572</td>\n",
       "        <td>0.93395</td>\n",
       "        <td>0.93122</td>\n",
       "        <td>10</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(table.get_html_string()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that the `f1` score for most of the alpha values are pretty comparable along with the `recall` but the `precision` drops a little in alpha = 10. Hence I feel that alpha = 1, the default value of alpha is good enough. Although lets see if the F1 score increases even more with higher alpha value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  20.1s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   38.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=  19.5s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  19.6s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  19.8s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  20.1s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  20.1s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  20.1s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  19.5s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  19.6s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  20.0s\n",
      "Done for MultinomialNB(alpha=50, class_prior=None, fit_prior=True)\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  6.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=  19.9s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   38.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=  19.9s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  19.8s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  19.9s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  19.8s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  19.9s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  19.7s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  20.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  19.8s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  19.9s\n",
      "Done for MultinomialNB(alpha=100, class_prior=None, fit_prior=True)\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  6.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=  19.9s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   38.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=  20.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  20.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  19.6s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  20.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  20.1s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  20.3s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  20.2s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  20.1s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  19.8s\n",
      "Done for MultinomialNB(alpha=1000, class_prior=None, fit_prior=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  6.5min finished\n"
     ]
    }
   ],
   "source": [
    "alphas = [50, 100, 1000]\n",
    "table2 = fit_params_with_ALPHA(alphas, features, labels, \n",
    "                              scorings, NUM_FOLDS, hyperparams=['alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>fit_time</th>\n",
       "        <th>score_time</th>\n",
       "        <th>train_accuracy</th>\n",
       "        <th>cv_accuracy</th>\n",
       "        <th>train_f1</th>\n",
       "        <th>cv_f1</th>\n",
       "        <th>train_recall</th>\n",
       "        <th>cv_recall</th>\n",
       "        <th>train_precision</th>\n",
       "        <th>cv_precision</th>\n",
       "        <th>ALPHA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>11.90005</td>\n",
       "        <td>7.9462</td>\n",
       "        <td>0.89059</td>\n",
       "        <td>0.88834</td>\n",
       "        <td>0.93798</td>\n",
       "        <td>0.93671</td>\n",
       "        <td>0.9722</td>\n",
       "        <td>0.97163</td>\n",
       "        <td>0.90608</td>\n",
       "        <td>0.90421</td>\n",
       "        <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>11.94663</td>\n",
       "        <td>7.93912</td>\n",
       "        <td>0.87561</td>\n",
       "        <td>0.87409</td>\n",
       "        <td>0.93119</td>\n",
       "        <td>0.93035</td>\n",
       "        <td>0.98905</td>\n",
       "        <td>0.98887</td>\n",
       "        <td>0.87973</td>\n",
       "        <td>0.87837</td>\n",
       "        <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>12.02611</td>\n",
       "        <td>7.97107</td>\n",
       "        <td>0.85103</td>\n",
       "        <td>0.85043</td>\n",
       "        <td>0.91952</td>\n",
       "        <td>0.91917</td>\n",
       "        <td>1.0</td>\n",
       "        <td>1.0</td>\n",
       "        <td>0.85103</td>\n",
       "        <td>0.85043</td>\n",
       "        <td>1000</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(table2.get_html_string()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm the trend is not consintent any more. We can see that the `cv_f1` decreases as the alpha value increased although the precison dropped continuously. For alpha 1000, recall is perfect but this ends up hurting precision a lot. We can see the effect in F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting the different evaluation metics for `Alpha = 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = PrettyTable(field_names=[\n",
    "    'test_accuracy',\n",
    "    'test_f1',\n",
    "    'test_recall',\n",
    "    'test_precision'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=1)\n",
    "clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(Dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = data_test.Polarity.apply(lambda x: 1 if x == 'positive' else 0).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta = round(accuracy_score(actual, preds), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8862"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = round(recall_score(actual, preds), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Precison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = round(precision_score(actual, preds), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1 = round(f1_score(actual, preds), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.add_row([ta, tf1, tr, tp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-------------+----------------+\n",
      "| test_accuracy | test_f1 | test_recall | test_precision |\n",
      "+---------------+---------+-------------+----------------+\n",
      "|     0.8862    | 0.93136 |   0.93555   |    0.92721     |\n",
      "|     0.8862    | 0.93136 |   0.93555   |    0.92721     |\n",
      "+---------------+---------+-------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12462</td>\n",
       "      <td>6621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5810</td>\n",
       "      <td>84343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1\n",
       "0  12462   6621\n",
       "1   5810  84343"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(actual, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-10.76626381, -10.6609033 , -10.52114135, ..., -11.17172892,\n",
       "        -10.61211313, -10.47858174],\n",
       "       [-10.79797896, -10.55989382, -10.38181856, ..., -10.07830502,\n",
       "        -10.1210923 , -10.01737247]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.feature_log_prob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_features(k):\n",
    "    neg_class_prob_sorted = clf.feature_log_prob_[0, :].argsort()\n",
    "    pos_class_prob_sorted = clf.feature_log_prob_[1, :].argsort()\n",
    "\n",
    "    print(np.take(cunt.get_feature_names(), neg_class_prob_sorted[:k]))\n",
    "    print(np.take(cunt.get_feature_names(), pos_class_prob_sorted[:k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['excellant' 'divide' 'stews' 'rebecca' 'addicting' 'peterson' 'pickiest'\n",
      " 'lends' 'accompaniment' 'nuke' 'joints' 'jitters' 'scrumptious'\n",
      " 'smoothest' 'yuban' 'gems' 'crock' 'backpack' 'breasts' 'perfection']\n",
      "['deceptive' 'moldy' 'disapointed' 'embarrassed' 'emails' 'lousy'\n",
      " 'defective' 'canceled' 'ashamed' 'bait' 'ruins' 'contacting' 'sorely'\n",
      " 'returns' 'inedible' 'horrid' 'contaminated' 'vaguely' 'hopeful' 'lesson']\n"
     ]
    }
   ],
   "source": [
    "top_k_features(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
